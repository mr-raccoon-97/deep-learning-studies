{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización de redes neuronales\n",
    "\n",
    "#### Covariable\n",
    "En estadística, una covariable es una variable que posiblemente predice el resultado bajo estudio. Pueden o no ser de importancia en el mismo. En las redes neuronales, las covariables son los covectores de entrada que se utilizan para predecir la variable de salida.\n",
    "\n",
    "Un problema comun en machine learning es el cambio de la distribución de las covariables en el conjunto de entrenamiento y las covariables con las que el modelo es puesto a prueba. Este problema es conocido como \"covariate shift\".\n",
    "\n",
    "En las redes neuronales profundas, este problema se da no solo a la diferencia entre los conjuntos de entrenamiento y prueba, sino tambien que tambien se da entre las capas de la red. Este problema es conocido como \"internal covariate shift\". Las covariables en las capas intermedias de la red neuronal cambian su distribución a medida que se actualizan los pesos de la red. Esto hace que el entrenamiento sea mas lento y dificil.\n",
    "\n",
    "Veamos un ejemplo de perceptrón multicapa.\n",
    "\n",
    "![image.png](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los circulos corresponden a las neuronas o covariables. Las flechas corresponden a los pesos de la red. Si cambiamos los pesos de una capa, la distribución de las covariables en la siguiente capa cambia, por lo que en cada iteración del entrenamiento, la siguiente capa se encuentra con una distribución diferente de covariables a las que aprendió en la iteración anterior.\n",
    "\n",
    "Esto causa dos problemas:\n",
    "- La red se entrena mas lentamente.\n",
    "- La red es inestable durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización por lote\n",
    "\n",
    "Para computar la normalización por bache, se calcula la media y la varianza de cada covariable en un bache de datos. Luego se normaliza cada covariable restando la media y dividiendo por la desviación estandar. Luego se escala y se traslada la covariable normalizada.\n",
    "\n",
    "Veamos esto:\n",
    "\n",
    "Sean $\\vec{x}^1, \\vec{x}^2, ..., \\vec{x}^m$  las covariables de una capa de la red neuronal de un lote de datos $X = \\begin{bmatrix} \\vec{x}^1 \\\\ \\vec{x}^2 \\\\ \\vdots \\\\ \\vec{x}^m \\end{bmatrix}$.\n",
    "\n",
    "Hallamos la media y desviación estandar del batch:\n",
    "\n",
    "$$\\vec{\\mu} = \\frac{1}{m} \\sum_{i=1}^{m} \\vec{x}^i \\quad \\quad \\quad \\vec{\\sigma}^2 = \\frac{1}{m} \\sum_{i=1}^{m} (\\vec{x}^i - \\vec{\\mu})^2$$\n",
    "\n",
    "Normalizamos las covariables:\n",
    "\n",
    "$$\\hat{\\vec{x}^i} = \\frac{\\vec{x}^i - \\vec{\\mu}}{\\sqrt{\\vec{\\sigma}^2 + \\epsilon}} \\quad \\quad \\quad \\hat{X} = \\begin{bmatrix} \\hat{\\vec{x}^1} \\\\ \\hat{\\vec{x}^2} \\\\ \\vdots \\\\ \\hat{\\vec{x}^m} \\end{bmatrix}$$\n",
    "\n",
    "Donde $\\epsilon$ es un valor pequeño para evitar la división por cero. Luego escalamos y trasladamos las covariables normalizadas, usando dos nuevos parámetros $\\gamma$ y $\\beta$ entrenables, finalmente obtenemos la normalización por lote:\n",
    "\n",
    "$$ \\text{BatchNorm}(X,\\gamma, \\beta)  = \\gamma \\odot \\hat{X} + \\beta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización por capa\n",
    "\n",
    "La normalización por capa es similar a la normalización por lote, pero en lugar de normalizar las covariables de un bache, se normalizan las covariables en si mismas. Es decir, se calcula la media y la varianza de cada covariable en una capa de la red neuronal. Luego se normaliza cada covariable restando la media y dividiendo por la desviación estandar. Luego se escala y se traslada la covariable normalizada.\n",
    "\n",
    "Veamos esto:\n",
    "\n",
    "Sean $\\vec{x}^1, \\vec{x}^2, ..., \\vec{x}^m$  las covariables de una capa de la red neuronal, en donde $d$ es la dimensión del espacio de caracteristicas. Hallamos la media y desviación estandar de cada caracteristica:\n",
    "\n",
    "$$\\vec{\\mu^i} = \\frac{1}{m} \\sum_{j=1}^{d} \\vec{x}_j^i \\quad \\quad \\quad \\vec{\\sigma^i}^2 = \\frac{1}{m} \\sum_{j=1}^{d} (\\vec{x}_j^i - \\vec{\\mu^i})^2$$\n",
    "\n",
    "Normalizamos las covariables:\n",
    "\n",
    "$$\\hat{\\vec{x}_j^i} = \\frac{\\vec{x}_j^i - \\vec{\\mu^i}}{\\sqrt{\\vec{\\sigma_i}^2 + \\epsilon}} \\quad \\quad \\quad \\hat{X} = \\begin{bmatrix} \\hat{\\vec{x}_1^1} & \\hat{\\vec{x}_2^1} & \\cdots & \\hat{\\vec{x}_d^1} \\\\ \\hat{\\vec{x}_1^2} & \\hat{\\vec{x}_2^2} & \\cdots & \\hat{\\vec{x}_d^2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\hat{\\vec{x}_1^m} & \\hat{\\vec{x}_2^m} & \\cdots & \\hat{\\vec{x}_d^m} \\end{bmatrix}$$\n",
    "\n",
    "Donde $\\epsilon$ es un valor pequeño para evitar la división por cero. Luego escalamos y trasladamos las covariables normalizadas, al igual que en la normalización por bache:\n",
    "\n",
    "$$ \\text{LayerNorm}(X,\\gamma, \\beta)  = \\gamma \\odot \\hat{X} + \\beta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización RMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
